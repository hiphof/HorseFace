{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunisian Horses - Only stills\n",
    "* do not use the cropped images, because the cropped images are most probably extracted from the stills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Prepare data for model training:\n",
    "# Base path to the database\n",
    "base_dir = '../data/THoDBRL2015'\n",
    "\n",
    "metadata = []\n",
    "\n",
    "# Consolidate images into a training directory\n",
    "output_train_dir = os.path.join(base_dir, 'training_data')\n",
    "\n",
    "os.makedirs(output_train_dir, exist_ok=True)\n",
    "\n",
    "parts = [\n",
    "    'Part1', \n",
    "    # 'Part2', \n",
    "    # 'Part3', \n",
    "    # 'Part4', \n",
    "    # 'Part5'\n",
    "]\n",
    "\n",
    "# Iterate through each Part folder\n",
    "for part in parts:\n",
    "    videos_dir = os.path.join(base_dir, part, 'videos')\n",
    "\n",
    "    for horse_id_folder in os.listdir(videos_dir):\n",
    "        horse_path = os.path.join(videos_dir, horse_id_folder)\n",
    "\n",
    "        if os.path.isdir(horse_path):  # Ensure it's a directory\n",
    "            # Find all stills folders (e.g., images, images1, images2, etc.)\n",
    "            for stills_folder in os.listdir(horse_path):\n",
    "                stills_path = os.path.join(horse_path, stills_folder)\n",
    "\n",
    "                if os.path.isdir(stills_path) and stills_folder.startswith('images'):  # Check for folders named 'images*'\n",
    "                    target_dir = os.path.join(output_train_dir, f'horse_{horse_id_folder}')\n",
    "\n",
    "                    os.makedirs(target_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Copy all image files from the stills folder to the target directory\n",
    "                    for img_file in os.listdir(stills_path):\n",
    "                        img_path = os.path.join(stills_path, img_file)\n",
    "\n",
    "                        if img_file.endswith(('.jpg', '.jpeg', '.png')):  # Ensure it's an image file\n",
    "                            metadata.append({\n",
    "                                'horse_id': horse_id_folder,\n",
    "                                'image_path': img_path,\n",
    "                                'width': Image.open(img_path).size[0],\n",
    "                                'height': Image.open(img_path).size[1],\n",
    "                            })\n",
    "                        \n",
    "                            # shutil.copy(img_path, target_dir)\n",
    "\n",
    "                    # print(f\"Copied images from {stills_path} to {target_dir}\")\n",
    "\n",
    "# Create DataFrame with specified dtypes\n",
    "metadata_df = pd.DataFrame(metadata, dtype='object').astype({\n",
    "    'horse_id': 'int64',          # Assuming it's an integer\n",
    "    'image_path': 'string',\n",
    "    'width': 'int64',\n",
    "    'height': 'int64',\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total images: {len(metadata_df)}\")\n",
    "print(f\"Total horses: {metadata_df['horse_id'].nunique()}\")\n",
    "print(f\"Width: {metadata_df['width'].unique()}\")\n",
    "print(f\"Height: {metadata_df['height'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata_df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_horse = metadata_df.groupby(['horse_id'], observed=True)\n",
    "\n",
    "grouped_horse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images per horse id\n",
    "grouped_horse.size().agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting data\n",
    "# Load data\n",
    "# Prepare dataset\n",
    "def load_images_and_labels(metadata_df):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        if os.path.exists(row['image_path']):\n",
    "            img = Image.open(row['image_path']).resize((128, 128))  # Scale to same size\n",
    "            images.append(np.array(img))\n",
    "            labels.append(row['horse_id'])  # use horse_id for the label\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images, labels = load_images_and_labels(metadata_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixelvalues\n",
    "# X_train = X_train / 255.0\n",
    "# X_test = X_test / 255.0\n",
    "\n",
    "# Transfer labels to category data\n",
    "# num_classes = len(metadata_df['horse_id'].unique())\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "y_train = y_train - 1  # If needed then custumize labels (sometimes gives error)\n",
    "y_test = y_test - 1  # The same for test\n",
    "\n",
    "# print(y_train.min())  # Min label value\n",
    "# print(y_train.max())  # Max label value\n",
    "\n",
    "# Convert labels to categorical data\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers step-by-step\n",
    "    model.add(keras.Input(shape=(128, 128, 3)))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # To prevent overfitting\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a model\n",
    "model = create_model()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings with epochs=10 and batch_size=32\n",
    "- expected time (335 sec * 10 epochs) ~~ 1 hour\n",
    "- Accuracy in 1st epoch: 0.54, and loss 3.92\n",
    "- Accuracy in 2nd epoch: on the start already accuracy of 0.91\n",
    "- In 2nd epoch, no big changes.\n",
    "\n",
    "Findings with epochs=1 and batch_size=64\n",
    "- expected time ~6 min (so same time per epoch)\n",
    "- accuracy is >0.95.\n",
    "\n",
    "Explanation:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save('../data/saved_models/my_model_epoch_1_batch_size_128.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [str(i) for i in range(metadata_df['horse_id'].nunique())]  \n",
    "\n",
    "# predict the labels for testset\n",
    "print(X_test.shape)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\n",
    "\n",
    "# Plot The confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Real Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM\n",
    "Use GradCAM to visualize the areas of the image that the model is looking at when making predictions.\n",
    "We use the last convolutional layer of the model as the last layer before the output layer.\n",
    "We use `tf-explain` to generate the GradCAM heatmap.\n",
    "\n",
    "If we have had used 'pytorch', we should have used 'torchcam' for this, which is making use of the 'hooks' in pyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential has never been called and thus has no defined output.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Start explainer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m explainer \u001b[38;5;241m=\u001b[39m GradCAM()\n\u001b[0;32m---> 26\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv2d_2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Save the GradCAM result\u001b[39;00m\n\u001b[1;32m     29\u001b[0m explainer\u001b[38;5;241m.\u001b[39msave(grid, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_cam.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/fontys/dasc2/HorseFace/venv_horseface/lib/python3.12/site-packages/tf_explain/core/grad_cam.py:54\u001b[0m, in \u001b[0;36mGradCAM.explain\u001b[0;34m(self, validation_data, model, class_index, layer_name, use_guided_grads, colormap, image_weight)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     layer_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer_grad_cam_target_layer(model)\n\u001b[0;32m---> 54\u001b[0m outputs, grads \u001b[38;5;241m=\u001b[39m \u001b[43mGradCAM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_gradients_and_filters\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_guided_grads\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m cams \u001b[38;5;241m=\u001b[39m GradCAM\u001b[38;5;241m.\u001b[39mgenerate_ponderated_output(outputs, grads)\n\u001b[1;32m     60\u001b[0m heatmaps \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[1;32m     61\u001b[0m     [\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# not showing the actual image if image_weight=0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m     ]\n\u001b[1;32m     66\u001b[0m )\n",
      "File \u001b[0;32m~/fontys/dasc2/HorseFace/venv_horseface/lib/python3.12/site-packages/tf_explain/core/grad_cam.py:111\u001b[0m, in \u001b[0;36mGradCAM.get_gradients_and_filters\u001b[0;34m(model, images, layer_name, class_index, use_guided_grads)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_gradients_and_filters\u001b[39m(\n\u001b[1;32m     95\u001b[0m     model, images, layer_name, class_index, use_guided_grads\n\u001b[1;32m     96\u001b[0m ):\n\u001b[1;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m    Generate guided gradients and convolutional outputs with an inference.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m        Tuple[tf.Tensor, tf.Tensor]: (Target layer outputs, Guided gradients)\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     grad_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(\n\u001b[0;32m--> 111\u001b[0m         [model\u001b[38;5;241m.\u001b[39minputs], [model\u001b[38;5;241m.\u001b[39mget_layer(layer_name)\u001b[38;5;241m.\u001b[39moutput, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m]\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m    115\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(images, tf\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/fontys/dasc2/HorseFace/venv_horseface/lib/python3.12/site-packages/keras/src/ops/operation.py:266\u001b[0m, in \u001b[0;36mOperation.output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moutput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the output tensor(s) of a layer.\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m        Output tensor or list of output tensors.\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fontys/dasc2/HorseFace/venv_horseface/lib/python3.12/site-packages/keras/src/ops/operation.py:285\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The layer sequential has never been called and thus has no defined output."
     ]
    }
   ],
   "source": [
    "# Load pretrained model or your own\n",
    "# model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=True)\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Assuming df is already defined and contains the image paths\n",
    "IMAGE_PATH = df['image_path'][0]\n",
    "\n",
    "# Load a sample image (or multiple ones)\n",
    "img = keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(128, 128))\n",
    "img = keras.preprocessing.image.img_to_array(img)\n",
    "img = np.expand_dims(img, axis=0)  # Expand to 4D tensor\n",
    "img = img / 255.0  # Normalize if required\n",
    "\n",
    "# Ensure the model is called with input data\n",
    "prediction = model.predict(img)\n",
    "class_index = np.argmax(prediction, axis=1)[0]  # Get the predicted class\n",
    "\n",
    "# Prepare data for GradCAM\n",
    "data = ([img], None)\n",
    "\n",
    "# Start explainer\n",
    "explainer = GradCAM()\n",
    "grid = explainer.explain(data, model, class_index=class_index, layer_name=\"conv2d_2\")\n",
    "\n",
    "# Save the GradCAM result\n",
    "explainer.save(grid, \".\", \"grad_cam.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_horseface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
