{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tunisian Horses - Only stills\n",
    "* do not use the cropped images, because the cropped images are most probably extracted from the stills"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. Prepare data for model training:\n",
    "# Base path to the database\n",
    "base_dir = '../data/THoDBRL2015'\n",
    "\n",
    "metadata = []\n",
    "\n",
    "# Consolidate images into a training directory\n",
    "output_train_dir = os.path.join(base_dir, 'training_data')\n",
    "\n",
    "os.makedirs(output_train_dir, exist_ok=True)\n",
    "\n",
    "parts = [\n",
    "    'Part1', \n",
    "    # 'Part2', \n",
    "    # 'Part3', \n",
    "    # 'Part4', \n",
    "    # 'Part5'\n",
    "]\n",
    "\n",
    "# Iterate through each Part folder\n",
    "for part in parts:\n",
    "    videos_dir = os.path.join(base_dir, part, 'videos')\n",
    "\n",
    "    for horse_id_folder in os.listdir(videos_dir):\n",
    "        horse_path = os.path.join(videos_dir, horse_id_folder)\n",
    "\n",
    "        if os.path.isdir(horse_path):  # Ensure it's a directory\n",
    "            # Find all stills folders (e.g., images, images1, images2, etc.)\n",
    "            for stills_folder in os.listdir(horse_path):\n",
    "                stills_path = os.path.join(horse_path, stills_folder)\n",
    "\n",
    "                if os.path.isdir(stills_path) and stills_folder.startswith('images'):  # Check for folders named 'images*'\n",
    "                    target_dir = os.path.join(output_train_dir, f'horse_{horse_id_folder}')\n",
    "\n",
    "                    os.makedirs(target_dir, exist_ok=True)\n",
    "                    \n",
    "                    # Copy all image files from the stills folder to the target directory\n",
    "                    for img_file in os.listdir(stills_path):\n",
    "                        img_path = os.path.join(stills_path, img_file)\n",
    "\n",
    "                        if img_file.endswith(('.jpg', '.jpeg', '.png')):  # Ensure it's an image file\n",
    "                            metadata.append({\n",
    "                                'horse_id': horse_id_folder,\n",
    "                                'image_path': img_path,\n",
    "                                'width': Image.open(img_path).size[0],\n",
    "                                'height': Image.open(img_path).size[1],\n",
    "                            })\n",
    "                        \n",
    "                            # shutil.copy(img_path, target_dir)\n",
    "\n",
    "                    # print(f\"Copied images from {stills_path} to {target_dir}\")\n",
    "\n",
    "# Create DataFrame with specified dtypes\n",
    "metadata_df = pd.DataFrame(metadata, dtype='object').astype({\n",
    "    'horse_id': 'int64',          # Assuming it's an integer\n",
    "    'image_path': 'string',\n",
    "    'width': 'int64',\n",
    "    'height': 'int64',\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total images: {len(metadata_df)}\")\n",
    "print(f\"Total horses: {metadata_df['horse_id'].nunique()}\")\n",
    "print(f\"Width: {metadata_df['width'].unique()}\")\n",
    "print(f\"Height: {metadata_df['height'].unique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata_df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_horse = metadata_df.groupby(['horse_id'], observed=True)\n",
    "\n",
    "grouped_horse.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images per horse id\n",
    "grouped_horse.size().agg(['min', 'max', 'mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For splitting data\n",
    "# Load data\n",
    "# Prepare dataset\n",
    "def load_images_and_labels(metadata_df):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in metadata_df.iterrows():\n",
    "        if os.path.exists(row['image_path']):\n",
    "            img = Image.open(row['image_path']).resize((128, 128))  # Scale to same size\n",
    "            images.append(np.array(img))\n",
    "            labels.append(row['horse_id'])  # use horse_id for the label\n",
    "    \n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return images, labels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "images, labels = load_images_and_labels(metadata_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42, stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixelvalues\n",
    "# X_train = X_train / 255.0\n",
    "# X_test = X_test / 255.0\n",
    "\n",
    "# Transfer labels to category data\n",
    "# num_classes = len(metadata_df['horse_id'].unique())\n",
    "num_classes = len(np.unique(labels))\n",
    "\n",
    "\n",
    "y_train = y_train - 1  # If needed then custumize labels (sometimes gives error)\n",
    "y_test = y_test - 1  # The same for test\n",
    "\n",
    "# print(y_train.min())  # Min label value\n",
    "# print(y_train.max())  # Max label value\n",
    "\n",
    "# Convert labels to categorical data\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalise pixel values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers step-by-step\n",
    "    model.add(keras.Input(shape=(128, 128, 3)))\n",
    "    \n",
    "    model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # To prevent overfitting\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))  # Output layer\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Create a model\n",
    "model = create_model()\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape\n",
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings with epochs=10 and batch_size=32\n",
    "- expected time (335 sec * 10 epochs) ~~ 1 hour\n",
    "- Accuracy in 1st epoch: 0.54, and loss 3.92\n",
    "- Accuracy in 2nd epoch: on the start already accuracy of 0.91\n",
    "- In 2nd epoch, no big changes.\n",
    "\n",
    "Findings with epochs=1 and batch_size=64\n",
    "- expected time ~6 min (so same time per epoch)\n",
    "- accuracy is >0.95.\n",
    "\n",
    "Explanation:\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model as a `.keras` zip archive.\n",
    "model.save('../data/saved_models/my_model_epoch_1_batch_size_128.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "class_names = [str(i) for i in range(metadata_df['horse_id'].nunique())]  \n",
    "\n",
    "# predict the labels for testset\n",
    "print(X_test.shape)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\n",
    "\n",
    "# Plot The confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('Real Class')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradCAM\n",
    "Use GradCAM to visualize the areas of the image that the model is looking at when making predictions.\n",
    "We use the last convolutional layer of the model as the last layer before the output layer.\n",
    "We use `tf-explain` to generate the GradCAM heatmap.\n",
    "\n",
    "If we have had used 'pytorch', we should have used 'torchcam' for this, which is making use of the 'hooks' in pyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained model or your own\n",
    "# model = tf.keras.applications.vgg16.VGG16(weights=\"imagenet\", include_top=True)\n",
    "from tf_explain.core.grad_cam import GradCAM\n",
    "\n",
    "IMAGE_PATH = df['image_path'][0]\n",
    "\n",
    "# Load a sample image (or multiple ones)\n",
    "img = keras.preprocessing.image.load_img(IMAGE_PATH, target_size=(128, 128))\n",
    "img = keras.preprocessing.image.img_to_array(img)\n",
    "data = ([img], None)\n",
    "\n",
    "# Start explainer\n",
    "explainer = GradCAM()\n",
    "images = np.expand_dims(img, axis=0) # Expand to 4D tensor\n",
    "print(images.shape)\n",
    "prediction = model(images)\n",
    "class_index = np.argmax(prediction, axis=1)[0] # Get the predicted class / horse_id\n",
    "grid = explainer.explain(data, model, class_index=class_index, layer_name=\"conv2d_2\")\n",
    "\n",
    "explainer.save(grid, \".\", \"grad_cam.png\")\n",
    "\n",
    "# Gives error:\n",
    "# AttributeError: The layer sequential has never been called and thus has no defined output.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_horseface",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
